{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inside-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import sklearn\n",
    "from sklearn import decomposition, cluster\n",
    "from sklearn.linear_model import LinearRegression as linreg\n",
    "\n",
    "\n",
    "import TwoPUtils\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indirect-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/mplitt/2P_scratch/all_session_dict.pkl\",\"rb\") as f:\n",
    "    all_sessions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incident-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '12_12_2020', 'scene': 'NeuroMods_LocationA', 'session': 2, 'scan': 2, 'mouse': 'GRABDA7', 'scan_file': '/media/mplitt/Backup Plus/GRABDA7/12_12_2020/NeuroMods_LocationA/NeuroMods_LocationA_002_002.sbx', 'scanheader_file': '/media/mplitt/Backup Plus/GRABDA7/12_12_2020/NeuroMods_LocationA/NeuroMods_LocationA_002_002.mat', 'vr_filename': '/home/mplitt/VR_scratch/GRABDA7/12_12_2020/NeuroMods_LocationA_2.sqlite', 'scan_number': 2, 'prompt_for_keys': False, 'VR_only': False, 'scanner': 'NLW', 's2p_path': '/mnt/BigDisk/2P_scratch/GRABDA7/12_12_2020/NeuroMods_LocationA/NeuroMods_LocationA_002_002/suite2p'}\n"
     ]
    }
   ],
   "source": [
    "mouse = \"GRABDA7\"\n",
    "ind = 2\n",
    "\n",
    "f = all_sessions[mouse][ind]\n",
    "f['mouse']=mouse\n",
    "\n",
    "\n",
    "scan_str = \"%s_%03d_%03d\" % (f['scene'],f['session'],f['scan'])\n",
    "try:\n",
    "    source_folder  = os.path.join('/media/mplitt','Backup Plus')\n",
    "    source_stem = os.path.join(source_folder,f['mouse'],f['date'],f['scene'],scan_str)\n",
    "    info = TwoPUtils.scanner_tools.sbx_utils.loadmat(source_stem+'.mat')\n",
    "except:\n",
    "    source_folder  = os.path.join('/media/mplitt','Backup Plus1','2P_Data')\n",
    "    source_stem = os.path.join(source_folder,f['mouse'],f['date'],f['scene'],scan_str)\n",
    "    info = TwoPUtils.scanner_tools.sbx_utils.loadmat(source_stem+'.mat')\n",
    "\n",
    "f.update({'scan_file': source_stem + '.sbx',\n",
    "          'scanheader_file': source_stem + '.mat',\n",
    "          'vr_filename': os.path.join(\"/home/mplitt/VR_scratch\",f['mouse'],f['date'],\"%s_%d.sqlite\" %(f['scene'],f['session'])),\n",
    "          'scan_number': f['scan'],\n",
    "          'prompt_for_keys': False,\n",
    "          'VR_only': False,\n",
    "          'scanner': \"NLW\",\n",
    "         })\n",
    "if f['mouse'] == 'GRABDA6':\n",
    "    f['s2p_path']=os.path.join(\"/home/mplitt/2P_scratch\",f['mouse'],f['date'],f['scene'],scan_str,'suite2p')\n",
    "else:\n",
    "    f['s2p_path']=os.path.join(\"/mnt/BigDisk/2P_scratch\",f['mouse'],f['date'],f['scene'],scan_str,'suite2p')\n",
    "    \n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "oriental-touch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mplitt/repos/TwoPUtils/sess.py:93: UserWarning: skipping checking keys, remaining initialization not guaranteed to work\n",
      "  warnings.warn(\"skipping checking keys, remaining initialization not guaranteed to work\")\n",
      "/home/mplitt/repos/TwoPUtils/sess.py:256: UserWarning: Looking for coaligned suite2p sessions is not implemented yet\n",
      "  warnings.warn(\"Looking for coaligned suite2p sessions is not implemented yet\")\n",
      "/home/mplitt/repos/TwoPUtils/preprocessing.py:134: RuntimeWarning: divide by zero encountered in remainder\n",
      "  lines = np.array([l % scan_info['fold_lines'] for l in scan_info['line']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1510.645780697322 1510.5811015664478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mplitt/repos/TwoPUtils/sess.py:390: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  v = v[np.newaxis, :]\n"
     ]
    }
   ],
   "source": [
    "sess = TwoPUtils.sess.Session(**f)\n",
    "sess.load_scan_info()\n",
    "sess.align_VR_to_2P()\n",
    "sess.load_suite2p_data()\n",
    "sess.add_timeseries(licks=sess.vr_data['lick'],rewards=sess.vr_data['reward'])\n",
    "sess.add_pos_binned_trial_matrix(['licks','rewards'],'pos')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legendary-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18070\n"
     ]
    }
   ],
   "source": [
    "# calculate place cells\n",
    "#regress green channel from red channel\n",
    "\n",
    "F = np.zeros(sess.timeseries['F'].shape)*np.nan\n",
    "F2 = np.zeros(sess.timeseries['F'].shape)*np.nan\n",
    "\n",
    "Fneu = np.zeros(sess.timeseries['F'].shape)*np.nan\n",
    "Fneu2 = np.zeros(sess.timeseries['F'].shape)*np.nan\n",
    "for i, (start,stop) in enumerate(zip(sess.trial_start_inds.tolist(),sess.teleport_inds.tolist())):\n",
    "    F[:,start-1:stop-1] = sess.timeseries['F'][:,start-1:stop-1]\n",
    "    F2[:,start-1:stop-1] = sess.timeseries['F_chan2'][:,start-1:stop-1]\n",
    "    \n",
    "    Fneu[:,start-1:stop-1] = sess.timeseries['Fneu'][:,start-1:stop-1]\n",
    "    Fneu2[:,start-1:stop-1] = sess.timeseries['Fneu_chan2'][:,start-1:stop-1]\n",
    "    \n",
    "nanmask = ~np.isnan(F[0,:])\n",
    "print(nanmask.sum())\n",
    "for cell in range(F.shape[0]):\n",
    "    lr = linreg().fit(F2[cell:cell+1,nanmask].T,F[cell,nanmask])\n",
    "    F[cell,nanmask] = F[cell,nanmask]-lr.predict(F2[cell:cell+1,nanmask].T) + lr.intercept_\n",
    "    \n",
    "    lr = linreg().fit(Fneu2[cell:cell+1,nanmask].T,Fneu[cell,nanmask])\n",
    "    Fneu[cell,nanmask] = Fneu[cell,nanmask]-lr.predict(Fneu2[cell:cell+1,nanmask].T) + lr.intercept_\n",
    "    \n",
    "F -= .7*Fneu\n",
    "F2 -= .7*Fneu2\n",
    "    \n",
    "\n",
    "\n",
    "Flow = sp.ndimage.filters.gaussian_filter(F[:,nanmask],    [0., 15])\n",
    "Flow = sp.ndimage.filters.minimum_filter1d(Flow,    int(500*15))\n",
    "Flow = sp.ndimage.filters.maximum_filter1d(Flow,    int(500*15))\n",
    "dFF = np.zeros(sess.timeseries['F'].shape)*np.nan\n",
    "dFF[:,nanmask] = (F[:,nanmask]-Flow)/np.abs(Flow)\n",
    "\n",
    "\n",
    "\n",
    "Flow = sp.ndimage.filters.gaussian_filter(F2[:,nanmask],    [0., 15])\n",
    "Flow = sp.ndimage.filters.minimum_filter1d(Flow,    int(500*15))\n",
    "Flow = sp.ndimage.filters.maximum_filter1d(Flow,    int(500*15))\n",
    "dFF2 = np.zeros(sess.timeseries['F'].shape)*np.nan\n",
    "dFF2[:,nanmask] = (F2[:,nanmask]-Flow)/np.abs(Flow)\n",
    "\n",
    "\n",
    "for i, (start,stop) in enumerate(zip(sess.trial_start_inds.tolist(),sess.teleport_inds.tolist())):\n",
    "    dFF[:,start-1:stop-1] = sp.ndimage.filters.gaussian_filter1d(dFF[:,start-1:stop-1],2,axis=1)\n",
    "    dFF2[:,start-1:stop-1] = sp.ndimage.filters.gaussian_filter1d(dFF2[:,start-1:stop-1],2,axis=1)\n",
    "\n",
    "sess.add_timeseries(dff=dFF, dff2=dFF2)\n",
    "sess.add_pos_binned_trial_matrix(['dff','dff2'],'pos')\n",
    "\n",
    "# for cell in range(0,dFF.shape[0],50):\n",
    "#     fig,ax = plt.subplots(figsize=[15,3])\n",
    "#     ax.plot(dFF[cell,:5000])\n",
    "#     ax.plot(dFF2[cell,:5000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "significant-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pca on average dopamine responses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reflected-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort cells by first PC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROI masks colored by "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
